{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import jieba\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\nimport warnings"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"# 读取停用词\nwith open('./resources/stop_words_zh_cn.txt', mode='r', encoding='utf-8') as stopword_f:\n    stopwords = list(map(lambda x: x.rstrip('\\n'), stopword_f.readlines()))\n# 去除网页转义符\nstopwords.extend(['nbsp','gt','lt','quot','amp'])\nstopwords = frozenset(stopwords)"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":"'一如既往地好吃，希望可以开到其他城市'"},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":"# 读取数据集\ncorpus = dataset = pd.read_csv('./resources/train.csv',sep=\"\\t\",names=[\"label\",\"comment\"],skiprows=1,encoding='utf-8')['comment'].tolist()\ncorpus[0]"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"Building prefix dict from the default dictionary ...\nLoading model from cache C:\\Users\\lonel\\AppData\\Local\\Temp\\jieba.cache\nLoading model cost 2.619 seconds.\nPrefix dict has been built succesfully.\n10000\n"},{"data":{"text/plain":"'一如既往 地 好吃 ， 希望 可以 开 到 其他 城市'"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":"# 切词\ncorpus = list(map(lambda x: ' '.join(jieba.cut(x)), corpus))\nprint(len(corpus))\ncorpus[0]"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":"array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":"# 统计词频\nwarnings.filterwarnings(\"ignore\")\ncount_vectorizer = CountVectorizer(stop_words=stopwords)\ncount_tf = count_vectorizer.fit_transform(corpus)\ncount_tf.toarray()"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":"['龙啸', '龙头', '龙抄手', '龙湖', '龙湾', '龙眼', '龙虾', '龙骨', '龟苓膏', '龟速']"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"# 所有的主题词\nfeature_names = count_vectorizer.get_feature_names()\nfeature_names[-10:]"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"topics = ['好评','差评']"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"# 进行 lda 转换\nlda = LatentDirichletAllocation(n_topics=len(topics),max_iter=10,learning_method=\"online\",\nlearning_offset=50,random_state=0)\ndocres = lda.fit_transform(count_tf)"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":"array([[2.21908771, 0.50721466, 0.51659712, ..., 1.20594492, 1.77215328,\n        1.40764435],\n       [0.50380219, 1.48958115, 1.5545252 , ..., 0.7296863 , 3.63065909,\n        0.51953256]])"},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":"# 每个主题每个词的重要指数\nlda_comp = lda.components_\nlda_comp"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[好评主题最重要主题词]:难吃,真的,好吃,东西,拉肚子,差评,味道,外卖,第一次,小时\n[差评主题最重要主题词]:味道,不错,好吃,服务,菜品,环境,喜欢,下次,分量,老板\n"}],"source":"# 每次主题的前10重要主题词\nfor i, word in enumerate(topics):\n    topic_comp = lda_comp[i]\n    indexed_topic_words = list(zip(range(len(topic_comp)),topic_comp))\n    indexed_topic_words = sorted(indexed_topic_words,key=lambda x: x[1],reverse=True)\n    most_important_topic_words = [feature_names[x[0]] for x in indexed_topic_words[:10]]\n    print(f\"[{word}主题最重要主题词]:{','.join(most_important_topic_words)}\")"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":"array([[0.12043239, 0.87956761],\n       [0.09808001, 0.90191999],\n       [0.05178284, 0.94821716],\n       ...,\n       [0.05764773, 0.94235227],\n       [0.0838649 , 0.9161351 ],\n       [0.08468626, 0.91531374]])"},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":"docres"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"味道,不错,好吃,服务,菜品,环境,喜欢,下次,分量,老板,服务态度,新鲜,特别,推荐,团购,热情,态度,几次,朋友,口味,划算,还会,价格,一如既往,感觉,实惠,位置,值得,蛋糕,干净\n"}],"source":"# 分析文章中重要性排名前30\nlongest_doc_id, max_len = 0, 0\nfor i,c in enumerate(corpus):\n    if len(c)>max_len:\n        max_len = len(c)\n        longest_doc_id = i\ntest_doc = corpus[longest_doc_id]\ndoc_bow = count_tf[longest_doc_id]\ndoc_topics = list(zip(feature_names,[docres[longest_doc_id,0] * lda_comp[0,i] + docres[longest_doc_id,1] * lda_comp[1,i] for i in range(len(feature_names))]))\ntop_words = [x[0] for x in sorted(doc_topics,key=lambda x: x[1], reverse=True)[:30]]\nprint(','.join(top_words))"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}